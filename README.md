<p align="center" style="background-color:#EBFFFF">
  <a href="https://xugaoyi.com/" target="_blank" rel="noopener noreferrer">
    <img width="180" src="https://cdn.jsdelivr.net/gh/Cynicism-lab/MyResource@gh-pages/img/hmprint.png" alt="logo">
  </a>
</p>

<h1 align="center">HMPrint</h1>
<p align="center">An efficient and advanced model for identity authentication using physiological features</p>

# Introduction
The system, called **_HMPrint_**, uses outer ear microphones to capture air friction-induced acoustic effects (AFiSe) generated by head movements for identity verification.

# Demo
This is a demo video including four head gestures about HMPrint, mentioned in the paper **_Decoding Air Friction Rhythms: Enabling User Identification with Out-Ear
Microphones in COTS Earphones_**


Click the link below to jump to the demo video playback



https://github.com/user-attachments/assets/33beadfb-0300-453d-a178-8f179a22e55a



We can see the system processing process in the terminal

**step1:** During the registration phase, the user performs four head gestures, each of which is repeated three times.

**step2:** The system processes the collected user registration data and uses it to train the model.

**step3:** The registered user performs three nod gestures and the verification is successful.

**step4:** The unregistered user(attacker) performs three nod gestures and the verification fails.













